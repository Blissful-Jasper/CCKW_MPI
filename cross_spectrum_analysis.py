"""
äº¤å‰è°±åˆ†æå·¥å…·æ¨¡å—
ç”¨äºè®¡ç®—å’Œå¯è§†åŒ–ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„äº¤å‰è°±å…³ç³»

Author: Refactored for generalization
Date: 2026-01-15
"""

import numpy as np
import xarray as xr
import os
import gc
from typing import Tuple, List, Dict, Optional, Union
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.axes import Axes
import psutil
import dask
import cmaps

from wave_tools import calculate_cross_spectrum, remove_annual_cycle
from wave_tools.utils import get_curve


# ============ å†…å­˜ç›‘æ§å·¥å…· ============
class MemoryMonitor:
    """å†…å­˜ç›‘æ§ç±»ï¼Œç”¨äºè·Ÿè¸ªç¨‹åºè¿è¡Œæ—¶çš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    
    def __init__(self):
        self.process = psutil.Process()
        
    def get_memory_info(self) -> Dict[str, float]:
        """
        è·å–å½“å‰å†…å­˜ä½¿ç”¨ä¿¡æ¯
        
        Returns:
        --------
        dict: åŒ…å«ä»¥ä¸‹é”®å€¼:
            - rss_gb: ç‰©ç†å†…å­˜ä½¿ç”¨é‡ (GB)
            - vms_gb: è™šæ‹Ÿå†…å­˜ä½¿ç”¨é‡ (GB)
            - percent: è¿›ç¨‹å†…å­˜å ç”¨ç™¾åˆ†æ¯”
            - available_gb: ç³»ç»Ÿå¯ç”¨å†…å­˜ (GB)
            - total_gb: ç³»ç»Ÿæ€»å†…å­˜ (GB)
        """
        mem = self.process.memory_info()
        mem_percent = self.process.memory_percent()
        virtual_mem = psutil.virtual_memory()
        
        return {
            'rss_gb': mem.rss / 1024**3,
            'vms_gb': mem.vms / 1024**3,
            'percent': mem_percent,
            'available_gb': virtual_mem.available / 1024**3,
            'total_gb': virtual_mem.total / 1024**3
        }
    
    def print_memory_status(self, label: str = "") -> Dict[str, float]:
        """
        æ‰“å°å†…å­˜çŠ¶æ€
        
        Parameters:
        -----------
        label : str
            æ ‡ç­¾ï¼Œç”¨äºæ ‡è¯†å½“å‰æ£€æŸ¥ç‚¹
            
        Returns:
        --------
        dict: å†…å­˜ä¿¡æ¯å­—å…¸
        """
        info = self.get_memory_info()
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ å†…å­˜çŠ¶æ€ {f'- {label}' if label else ''}")
        print(f"{'='*60}")
        print(f"  è¿›ç¨‹ç‰©ç†å†…å­˜ä½¿ç”¨: {info['rss_gb']:.2f} GB")
        print(f"  è¿›ç¨‹è™šæ‹Ÿå†…å­˜ä½¿ç”¨: {info['vms_gb']:.2f} GB")
        print(f"  è¿›ç¨‹å†…å­˜å æ¯”: {info['percent']:.1f}%")
        print(f"  ç³»ç»Ÿå¯ç”¨å†…å­˜: {info['available_gb']:.2f} GB / {info['total_gb']:.2f} GB")
        print(f"{'='*60}\n")
        
        if info['available_gb'] < 10:
            print("âš ï¸  è­¦å‘Š: ç³»ç»Ÿå¯ç”¨å†…å­˜ä¸è¶³10GB!")
        
        return info


# ============ æ•°æ®åŠ è½½å·¥å…· ============
def load_netcdf_data(
    file_path: str,
    chunks: Optional[Dict[str, int]] = None,
    verbose: bool = True
) -> xr.DataArray:
    """
    åŠ è½½NetCDFæ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒlazy loadingï¼‰
    
    Parameters:
    -----------
    file_path : str
        æ•°æ®æ–‡ä»¶è·¯å¾„
    chunks : dict, optional
        æ•°æ®åˆ†å—å‚æ•°ï¼Œç”¨äºdask lazy loading
        ä¾‹å¦‚: {'time': 5000}
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    xr.DataArray
        åŠ è½½çš„æ•°æ®ï¼ˆå¦‚æœæŒ‡å®šchunksåˆ™ä¸ºlazy loadingï¼‰
        
    Raises:
    -------
    FileNotFoundError
        å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if verbose:
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {file_path}")
    
    # é»˜è®¤ä½¿ç”¨æ—¶é—´ç»´åº¦åˆ†å—
    if chunks is None:
        chunks = {'time': 5000}
    
    # å°è¯•ä¸åŒçš„å¼•æ“åŠ è½½æ•°æ®
    for engine in ['netcdf4', 'h5netcdf', None]:
        try:
            if engine:
                data = xr.open_dataarray(file_path, engine=engine, chunks=chunks)
            else:
                data = xr.open_dataarray(file_path, chunks=chunks)
            break
        except Exception as e:
            if engine is None:
                raise e
            continue
    
    if verbose:
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ (lazy loading)")
        print(f"   å½¢çŠ¶: {data.shape}")
        print(f"   ç»´åº¦: {data.dims}")
        if hasattr(data, 'chunks'):
            print(f"   æ•°æ®å—: {data.chunks}")
        if hasattr(data, 'time'):
            print(f"   æ—¶é—´èŒƒå›´: {data.time.values[0]} to {data.time.values[-1]}")
        data_size_gb = data.nbytes / 1024**3
        print(f"   ä¼°ç®—æ•°æ®å¤§å°: {data_size_gb:.2f} GB")
    
    return data


def load_multiple_experiments(
    variable_name: str,
    experiments: List[str],
    data_dir: str,
    file_pattern: str = "{var}_{exp}_2deg_interp.nc",
    chunks: Optional[Dict[str, int]] = None,
    scale_factor: float = 1.0,
    verbose: bool = True
) -> Dict[str, xr.DataArray]:
    """
    åŠ è½½å¤šä¸ªå®éªŒçš„åŒä¸€å˜é‡æ•°æ®
    
    Parameters:
    -----------
    variable_name : str
        å˜é‡åç§°ï¼ˆç”¨äºæ„å»ºæ–‡ä»¶åï¼‰
    experiments : list of str
        å®éªŒåç§°åˆ—è¡¨ï¼Œä¾‹å¦‚: ['cntl', 'p4k', '4co2']
    data_dir : str
        æ•°æ®ç›®å½•è·¯å¾„
    file_pattern : str
        æ–‡ä»¶åæ¨¡æ¿ï¼Œä½¿ç”¨{var}å’Œ{exp}ä½œä¸ºå ä½ç¬¦
    chunks : dict, optional
        æ•°æ®åˆ†å—å‚æ•°
    scale_factor : float
        æ•°æ®ç¼©æ”¾å› å­ï¼ˆä¾‹å¦‚ï¼Œæ½œçƒ­é€šé‡å¯èƒ½éœ€è¦ä¹˜ä»¥-1ï¼‰
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    dict
        é”®ä¸ºå®éªŒåï¼Œå€¼ä¸ºå¯¹åº”çš„DataArray
        
    Example:
    --------
    >>> pr_data = load_multiple_experiments(
    ...     variable_name='pr',
    ...     experiments=['cntl', 'p4k', '4co2'],
    ...     data_dir='/path/to/data'
    ... )
    """
    if verbose:
        print(f"\n{'='*60}")
        print(f"åŠ è½½å˜é‡: {variable_name.upper()}")
        print(f"{'='*60}")
    
    data_dict = {}
    
    for exp in experiments:
        try:
            file_path = os.path.join(
                data_dir,
                file_pattern.format(var=variable_name, exp=exp)
            )
            
            if verbose:
                print(f"\nåŠ è½½ {exp.upper()}...")
            
            data = load_netcdf_data(file_path, chunks=chunks, verbose=verbose)
            
            # åº”ç”¨ç¼©æ”¾å› å­
            if scale_factor != 1.0:
                data = data * scale_factor
                if verbose:
                    print(f"   åº”ç”¨ç¼©æ”¾å› å­: {scale_factor}")
            
            data_dict[exp] = data
            
        except Exception as e:
            print(f"âŒ åŠ è½½ {exp} å¤±è´¥: {e}")
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data_dict)} ä¸ªå®éªŒçš„æ•°æ®")
        print(f"{'='*60}")
    
    return data_dict


# ============ æ•°æ®é¢„å¤„ç†å·¥å…· ============
def preprocess_data_with_mask(
    data1: xr.DataArray,
    data2: xr.DataArray,
    mask: Optional[xr.DataArray] = None,
    remove_annual: bool = True,
    fill_value: float = 0.0,
    verbose: bool = True
) -> Tuple[xr.DataArray, xr.DataArray]:
    """
    é¢„å¤„ç†ä¸¤ä¸ªæ•°æ®æ•°ç»„ï¼šå»é™¤å¹´å¾ªç¯ã€åº”ç”¨æ©è†œã€æ¸…ç†æ— æ•ˆå€¼
    
    Parameters:
    -----------
    data1, data2 : xr.DataArray
        è¾“å…¥æ•°æ®ï¼ˆæ”¯æŒdask lazy loadingï¼‰
    mask : xr.DataArray, optional
        ç©ºé—´æ©è†œï¼ˆä¾‹å¦‚æµ·æ´‹æ©è†œï¼‰ï¼ŒTrueè¡¨ç¤ºä¿ç•™ï¼ŒFalseè¡¨ç¤ºå±è”½
    remove_annual : bool
        æ˜¯å¦å»é™¤å¹´å¾ªç¯
    fill_value : float
        å¡«å……NaNçš„å€¼
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    data1_processed, data2_processed : xr.DataArray
        é¢„å¤„ç†åçš„æ•°æ®ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰
    """
    if verbose:
        print("\n" + "="*60)
        print("æ•°æ®é¢„å¤„ç†")
        print("="*60)
    
    # æ­¥éª¤1: å»é™¤å¹´å¾ªç¯
    if remove_annual:
        if verbose:
            print("  ğŸ“Š æ­¥éª¤1: å»é™¤å¹´å¾ªç¯ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰...")
        data1_ano = data1.groupby('time.dayofyear') - data1.groupby('time.dayofyear').mean()
        data2_ano = data2.groupby('time.dayofyear') - data2.groupby('time.dayofyear').mean()
    else:
        data1_ano = data1
        data2_ano = data2
    
    # æ­¥éª¤2: åº”ç”¨æ©è†œ
    if mask is not None:
        if verbose:
            print("  ğŸŒŠ æ­¥éª¤2: åº”ç”¨æ©è†œï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰...")
        mask_float = mask.astype(float)
        data1_ano = data1_ano * mask_float
        data2_ano = data2_ano * mask_float
    
    # æ­¥éª¤3: æ¸…ç†æ— æ•ˆå€¼
    if verbose:
        print("  ğŸ§¹ æ­¥éª¤3: æ¸…ç†æ— æ•ˆå€¼ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰...")
    
    # å¤„ç†Inf
    data1_ano = xr.where(np.isinf(data1_ano), np.nan, data1_ano)
    data2_ano = xr.where(np.isinf(data2_ano), np.nan, data2_ano)
    
    # å¡«å……NaN
    data1_ano = data1_ano.fillna(fill_value)
    data2_ano = data2_ano.fillna(fill_value)
    
    # æ­¥éª¤4: å†æ¬¡å»é™¤å¹´å¾ªç¯ï¼ˆä½¿ç”¨wave_toolså‡½æ•°ï¼‰
    if remove_annual:
        if verbose:
            print("  ğŸ”„ æ­¥éª¤4: å†æ¬¡å»é™¤å¹´å¾ªç¯ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰...")
        data1_ano = remove_annual_cycle(data1_ano)
        data2_ano = remove_annual_cycle(data2_ano)
    
    if verbose and hasattr(data1_ano, 'chunks'):
        print(f"     data1 chunks: {data1_ano.chunks}")
        print(f"     data2 chunks: {data2_ano.chunks}")
    
    return data1_ano, data2_ano


# ============ äº¤å‰è°±è®¡ç®—å·¥å…· ============
def compute_cross_spectrum_for_experiments(
    data1_dict: Dict[str, xr.DataArray],
    data2_dict: Dict[str, xr.DataArray],
    experiments: List[str],
    mask: Optional[xr.DataArray] = None,
    seg_length: int = 96,
    seg_overlap: int = -65,
    symmetry: str = 'symm',
    memory_monitor: Optional[MemoryMonitor] = None,
    verbose: bool = True
) -> Dict[str, Dict]:
    """
    ä¸ºå¤šä¸ªå®éªŒè®¡ç®—äº¤å‰è°±
    
    Parameters:
    -----------
    data1_dict, data2_dict : dict
        é”®ä¸ºå®éªŒåï¼Œå€¼ä¸ºDataArrayçš„å­—å…¸
    experiments : list of str
        è¦å¤„ç†çš„å®éªŒååˆ—è¡¨
    mask : xr.DataArray, optional
        ç©ºé—´æ©è†œ
    seg_length : int
        åˆ†æ®µé•¿åº¦ï¼ˆå¤©æ•°ï¼‰
    seg_overlap : int
        åˆ†æ®µé‡å é•¿åº¦
    symmetry : str
        å¯¹ç§°æ€§è®¾ç½®: 'symm', 'asymm', 'latband'
    memory_monitor : MemoryMonitor, optional
        å†…å­˜ç›‘æ§å™¨å®ä¾‹
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    dict
        åµŒå¥—å­—å…¸ï¼Œç»“æ„ä¸º:
        {
            'exp_name': {
                'STC': xr.DataArray,  # è°±åˆ†é‡
                'freq': np.ndarray,    # é¢‘ç‡
                'wave': np.ndarray,    # æ³¢æ•°
                'nseg': int,           # åˆ†æ®µæ•°
                'dof': int,            # è‡ªç”±åº¦
                'p': float,            # på€¼
                'prob_coh2': float     # coherence squaredä¸´ç•Œå€¼
            }
        }
    """
    results = {}
    
    for exp_name in experiments:
        if verbose:
            print(f"\n{'='*60}")
            print(f"å¤„ç†å®éªŒ: {exp_name.upper()}")
            print(f"{'='*60}")
        
        if memory_monitor:
            memory_monitor.print_memory_status(f"å¼€å§‹å¤„ç† {exp_name}")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
        if exp_name not in data1_dict or exp_name not in data2_dict:
            print(f"âš ï¸  è­¦å‘Š: å®éªŒ {exp_name} çš„æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
            continue
        
        # è·å–åŸå§‹æ•°æ®
        data1_raw = data1_dict[exp_name]
        data2_raw = data2_dict[exp_name]
        
        # åº”ç”¨æ©è†œï¼ˆå¦‚æœæä¾›ï¼‰
        if mask is not None:
            data1_raw = data1_raw.where(mask, drop=True)
            data2_raw = data2_raw.where(mask, drop=True)
        
        if verbose and hasattr(data1_raw, 'chunks'):
            print(f"  ğŸ“¦ åŸå§‹æ•°æ® chunks:")
            print(f"     data1: {data1_raw.chunks}")
            print(f"     data2: {data2_raw.chunks}")
        
        # é¢„å¤„ç†æ•°æ®
        data1_ano, data2_ano = preprocess_data_with_mask(
            data1_raw, data2_raw, mask=None, verbose=verbose
        )
        
        # è®¡ç®—æ•°æ®ï¼ˆDaskå»¶è¿Ÿè®¡ç®—ï¼‰
        if verbose:
            print("\n  ğŸ’» æ‰§è¡ŒDaskè®¡ç®—...")
        data1_computed, data2_computed = dask.compute(data1_ano, data2_ano)
        
        if memory_monitor:
            memory_monitor.print_memory_status(f"é¢„å¤„ç†å®Œæˆ {exp_name}")
        
        # è®¡ç®—äº¤å‰è°±
        if verbose:
            print(f"\n  ğŸ“Š è®¡ç®—äº¤å‰è°±...")
            print(f"     åˆ†æ®µé•¿åº¦: {seg_length}")
            print(f"     åˆ†æ®µé‡å : {seg_overlap}")
            print(f"     å¯¹ç§°æ€§: {symmetry}")
        
        result = calculate_cross_spectrum(
            data1_computed, data2_computed,
            segLen=seg_length,
            segOverLap=seg_overlap,
            symmetry=symmetry,
            return_xarray=True
        )
        
        # æ£€æŸ¥ç»“æœ
        if result is None:
            print(f"    âœ— äº¤å‰è°±è®¡ç®—å¤±è´¥")
            continue
        
        # ä¿å­˜ç»“æœ
        results[exp_name] = {
            'STC': result['STC'],
            'freq': result['freq'],
            'wave': result['wave'],
            'nseg': result['nseg'],
            'dof': result['dof'],
            'p': result['p'],
            'prob_coh2': result['prob_coh2']
        }
        
        if verbose:
            print(f"    âœ“ äº¤å‰è°±è®¡ç®—å®Œæˆ")
            print(f"      åˆ†æ®µæ•°: {result['nseg']}")
            print(f"      è‡ªç”±åº¦: {result['dof']}")
            print(f"      99%æ˜¾è‘—æ€§é˜ˆå€¼: {result['prob_coh2']}")
        
        if memory_monitor:
            memory_monitor.print_memory_status(f"å®Œæˆ {exp_name}")
        
        # æ¸…ç†å†…å­˜
        del data1_ano, data2_ano, data1_computed, data2_computed
        gc.collect()
    
    return results


# ============ å¯è§†åŒ–å·¥å…· ============
def plot_cross_spectrum_panel(
    results: Dict[str, Dict],
    experiments: List[str],
    exp_titles: Optional[List[str]] = None,
    figsize: Tuple[float, float] = (16, 8),
    dpi: int = 300,
    significance_level: float = 0.99,
    cmap: str = 'WhiteBlueGreenYellowRed',
    contour_levels: Optional[np.ndarray] = None,
    vector_scale: float = 30,
    vector_skip: int = 2,
    xlim: Tuple[float, float] = (-15, 15),
    ylim: Tuple[float, float] = (0, 0.5),
    add_dispersion_curves: bool = True,
    add_period_lines: bool = True,
    period_days: List[int] = [3, 6, 20],
    equivalent_depths: List[int] = [8, 25, 90],
    output_path: Optional[str] = None,
    verbose: bool = True
) -> Tuple[Figure, np.ndarray]:
    """
    ç»˜åˆ¶äº¤å‰è°±åˆ†æçš„é¢æ¿å›¾
    
    Parameters:
    -----------
    results : dict
        compute_cross_spectrum_for_experimentsçš„è¾“å‡ºç»“æœ
    experiments : list of str
        è¦ç»˜åˆ¶çš„å®éªŒåˆ—è¡¨
    exp_titles : list of str, optional
        å®éªŒçš„æ˜¾ç¤ºæ ‡é¢˜ï¼Œé»˜è®¤ä½¿ç”¨å¤§å†™çš„å®éªŒå
    figsize : tuple
        å›¾åƒå¤§å° (width, height)
    dpi : int
        å›¾åƒåˆ†è¾¨ç‡
    significance_level : float
        æ˜¾è‘—æ€§æ°´å¹³ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œç”¨äºæ©è”½ä¸æ˜¾è‘—çš„ç»“æœ
    cmap : str
        è‰²æ ‡åç§°ï¼ˆcmapsåŒ…çš„è‰²æ ‡ï¼‰
    contour_levels : np.ndarray, optional
        ç­‰é«˜çº¿æ°´å¹³ï¼Œé»˜è®¤ä¸ºnp.linspace(0.2, 0.8, 21)
    vector_scale : float
        çŸ¢é‡ç¼©æ”¾å› å­
    vector_skip : int
        çŸ¢é‡åœºé‡‡æ ·é—´éš”
    xlim, ylim : tuple
        xå’Œyè½´èŒƒå›´
    add_dispersion_curves : bool
        æ˜¯å¦æ·»åŠ Kelvinæ³¢è‰²æ•£å…³ç³»æ›²çº¿
    add_period_lines : bool
        æ˜¯å¦æ·»åŠ å‘¨æœŸå‚è€ƒçº¿
    period_days : list of int
        è¦æ ‡æ³¨çš„å‘¨æœŸï¼ˆå¤©ï¼‰
    equivalent_depths : list of int
        Kelvinæ³¢ç­‰æ•ˆæ·±åº¦ï¼ˆç±³ï¼‰
    output_path : str, optional
        è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜å›¾åƒ
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    fig : matplotlib.figure.Figure
        å›¾åƒå¯¹è±¡
    axes : np.ndarray
        å­å›¾æ•°ç»„
    """
    # å‚æ•°æ£€æŸ¥
    n_exp = len(experiments)
    if exp_titles is None:
        exp_titles = [exp.upper() for exp in experiments]
    
    if len(exp_titles) != n_exp:
        raise ValueError("exp_titlesé•¿åº¦å¿…é¡»ä¸experimentsä¸€è‡´")
    
    if contour_levels is None:
        contour_levels = np.linspace(0.2, 0.8, 21)
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(1, n_exp, figsize=figsize, dpi=dpi)
    if n_exp == 1:
        axes = np.array([axes])
    
    plt.subplots_adjust(left=0.06, right=0.98, top=0.92, bottom=0.15, wspace=0.25)
    plt.rcParams.update({'font.size': 10})
    
    # å¸¸æ•°
    s2d = 86400  # ç§’/å¤©
    earth_radius = 6371 * 1000  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰
    
    # ç»˜åˆ¶æ¯ä¸ªå®éªŒ
    for idx, (exp_name, exp_title, ax) in enumerate(zip(experiments, exp_titles, axes)):
        if exp_name not in results:
            if verbose:
                print(f"âš ï¸  è­¦å‘Š: ç»“æœä¸­ä¸åŒ…å«å®éªŒ {exp_name}ï¼Œè·³è¿‡")
            continue
        
        plt.sca(ax)
        
        # è·å–æ•°æ®
        stc = results[exp_name]['STC']
        wave = results[exp_name]['wave']
        freq = results[exp_name]['freq']
        prob_coh2 = results[exp_name]['prob_coh2']
        
        # è·å–æ˜¾è‘—æ€§é˜ˆå€¼
        if isinstance(prob_coh2, np.ndarray) and prob_coh2.size > 1:
            threshold = float(prob_coh2.max())
        elif hasattr(prob_coh2, 'item'):
            threshold = float(prob_coh2.item())
        else:
            threshold = float(prob_coh2)
        
        if verbose:
            print(f"\n{exp_name.upper()}:")
            print(f"  æ˜¾è‘—æ€§é˜ˆå€¼ ({int(significance_level*100)}%): {threshold:.6f}")
        
        # è·å–coherence squared
        coh2 = stc.sel(component='COH2')
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§
        n_significant = int((coh2 >= threshold).sum())
        if verbose:
            print(f"  æ˜¾è‘—ç‚¹æ•°: {n_significant}/{coh2.size}")
        
        # æ©è”½ä¸æ˜¾è‘—çš„æ•°æ®
        coh2_masked = coh2.where(coh2 >= threshold)
        
        # ç»˜åˆ¶ç­‰é«˜çº¿
        try:
            contourf = coh2_masked.plot.contourf(
                ax=ax,
                cmap=getattr(cmaps, cmap) if hasattr(cmaps, cmap) else cmap,
                levels=contour_levels,
                add_colorbar=False,
                add_labels=False,
                extend='neither'
            )
        except Exception as e:
            print(f"âš ï¸  ç»˜åˆ¶ç­‰é«˜çº¿æ—¶å‡ºé”™: {e}")
            contourf = None
        
        # å‡†å¤‡çŸ¢é‡åœº
        wave_sub = wave[::vector_skip]
        freq_sub = freq[::vector_skip]
        u_sub = stc.sel(component='V1').values[::vector_skip, ::vector_skip]
        v_sub = stc.sel(component='V2').values[::vector_skip, ::vector_skip]
        coh2_sub = stc.sel(component='COH2').values[::vector_skip, ::vector_skip]
        
        # æ©è”½çŸ¢é‡åœº
        mask = coh2_sub < threshold
        u_masked = np.where(mask, np.nan, u_sub)
        v_masked = np.where(mask, np.nan, v_sub)
        
        n_valid = np.sum(~np.isnan(u_masked))
        if verbose:
            print(f"  æœ‰æ•ˆçŸ¢é‡æ•°: {n_valid}")
        
        # ç»˜åˆ¶çŸ¢é‡
        if n_valid > 0:
            ax.quiver(
                wave_sub, freq_sub,
                u_masked, v_masked,
                scale=vector_scale, headwidth=4, headlength=5,
                width=0.004, alpha=0.8
            )
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title(f'({chr(97 + idx)}) {exp_title}', fontsize=18, loc='left')
        ax.set_title(f'Sym', fontsize=10, loc='right')
        
        # è®¾ç½®åæ ‡è½´
        ax.set_ylabel('Frequency (1/day)', fontsize=18)
        ax.set_xlabel('Zonal wavenumber', fontsize=18)
        ax.set_xlim(xlim)
        ax.set_ylim(ylim)
        
        # æ·»åŠ CCKWæ³¢æ®µ
        try:
            kw_x, kw_y = get_curve()
            ax.plot(kw_x[0], kw_y[0], 'red', linewidth=1.5, 
                   linestyle='solid', label='CCKW band')
        except:
            pass
        
        # æ·»åŠ å‘¨æœŸçº¿
        if add_period_lines:
            ax.plot([0, 0], ylim, 'k', linewidth=1, linestyle=':')
            
            for day, label in zip(period_days, [f'{d}d' for d in period_days]):
                period_freq = 1 / day
                ax.plot(xlim, [period_freq, period_freq], 'k', 
                       linewidth=1, linestyle=':')
                ax.text(xlim[0] + 0.2, period_freq + 0.01, label, 
                       fontsize=15, color='k')
        
        # æ·»åŠ è‰²æ•£å…³ç³»
        if add_dispersion_curves:
            cp = (9.8 * np.array(equivalent_depths)) ** 0.5
            wave_goal = 0.5 / s2d / cp * 2 * np.pi * earth_radius
            
            for wg in wave_goal:
                ax.plot([0, wg], [0, ylim[1]], 'grey', 
                       linewidth=1, linestyle='dashed')
            
            ax.text(12, 0.35, 'kelvin', ha="center", va="center", size=9,
                   bbox={'facecolor': 'w', 'alpha': 0.9, 'edgecolor': 'none'})
        
        ax.tick_params(labelsize=18, which='both', top=True, right=True)
    
    # æ·»åŠ è‰²æ ‡
    if contourf is not None:
        cbar = fig.colorbar(contourf, ax=axes, orientation='horizontal',
                           pad=0.15, aspect=40, shrink=0.8)
        cbar.set_label('Coherence Squared', fontsize=14)
    
    # ä¿å­˜å›¾åƒ
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        fig.savefig(output_path, bbox_inches='tight')
        if verbose:
            print(f"\nâœ… å›¾åƒå·²ä¿å­˜: {output_path}")
    
    return fig, axes


# ============ ä¾¿æ·å‡½æ•° ============
def analyze_cross_spectrum(
    var1_name: str,
    var2_name: str,
    experiments: List[str],
    data_dir: str,
    mask: Optional[xr.DataArray] = None,
    file_pattern: str = "{var}_{exp}_2deg_interp.nc",
    var1_scale: float = 1.0,
    var2_scale: float = 1.0,
    chunks: Optional[Dict[str, int]] = None,
    seg_length: int = 96,
    seg_overlap: int = -65,
    symmetry: str = 'symm',
    output_dir: Optional[str] = None,
    plot_params: Optional[Dict] = None,
    verbose: bool = True
) -> Tuple[Dict[str, Dict], Optional[Tuple[Figure, np.ndarray]]]:
    """
    ä¸€ç«™å¼äº¤å‰è°±åˆ†æï¼šä»æ•°æ®åŠ è½½åˆ°ç»“æœå¯è§†åŒ–
    
    Parameters:
    -----------
    var1_name, var2_name : str
        ä¸¤ä¸ªå˜é‡çš„åç§°
    experiments : list of str
        å®éªŒåç§°åˆ—è¡¨
    data_dir : str
        æ•°æ®ç›®å½•
    mask : xr.DataArray, optional
        ç©ºé—´æ©è†œ
    file_pattern : str
        æ–‡ä»¶åæ¨¡æ¿
    var1_scale, var2_scale : float
        å˜é‡ç¼©æ”¾å› å­
    chunks : dict, optional
        daskåˆ†å—å‚æ•°
    seg_length : int
        è°±åˆ†æåˆ†æ®µé•¿åº¦
    seg_overlap : int
        åˆ†æ®µé‡å 
    symmetry : str
        å¯¹ç§°æ€§
    output_dir : str, optional
        è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜å›¾åƒï¼‰
    plot_params : dict, optional
        ä¼ é€’ç»™plot_cross_spectrum_panelçš„é¢å¤–å‚æ•°
    verbose : bool
        æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
    --------
    results : dict
        äº¤å‰è°±è®¡ç®—ç»“æœ
    fig_axes : tuple or None
        (fig, axes)å…ƒç»„ï¼Œå¦‚æœæŒ‡å®šäº†output_dir
        
    Example:
    --------
    >>> results, (fig, axes) = analyze_cross_spectrum(
    ...     var1_name='pr',
    ...     var2_name='olr',
    ...     experiments=['cntl', 'p4k', '4co2'],
    ...     data_dir='/path/to/data',
    ...     mask=ocean_mask,
    ...     output_dir='./figures/cross_spectrum'
    ... )
    """
    # åˆ›å»ºå†…å­˜ç›‘æ§å™¨
    mem_mon = MemoryMonitor()
    mem_mon.print_memory_status("åˆ†æå¼€å§‹")
    
    # åŠ è½½æ•°æ®
    if verbose:
        print(f"\n{'='*60}")
        print(f"äº¤å‰è°±åˆ†æ: {var1_name.upper()} vs {var2_name.upper()}")
        print(f"{'='*60}")
    
    var1_data = load_multiple_experiments(
        var1_name, experiments, data_dir,
        file_pattern=file_pattern,
        chunks=chunks,
        scale_factor=var1_scale,
        verbose=verbose
    )
    
    var2_data = load_multiple_experiments(
        var2_name, experiments, data_dir,
        file_pattern=file_pattern,
        chunks=chunks,
        scale_factor=var2_scale,
        verbose=verbose
    )
    
    # è®¡ç®—äº¤å‰è°±
    results = compute_cross_spectrum_for_experiments(
        var1_data, var2_data,
        experiments=experiments,
        mask=mask,
        seg_length=seg_length,
        seg_overlap=seg_overlap,
        symmetry=symmetry,
        memory_monitor=mem_mon,
        verbose=verbose
    )
    
    # å¯è§†åŒ–
    fig_axes = None
    if output_dir:
        if plot_params is None:
            plot_params = {}
        
        output_path = os.path.join(
            output_dir,
            f'cross_spectrum_{var1_name}_{var2_name}_{symmetry}.png'
        )
        
        fig, axes = plot_cross_spectrum_panel(
            results,
            experiments=experiments,
            output_path=output_path,
            verbose=verbose,
            **plot_params
        )
        fig_axes = (fig, axes)
    
    mem_mon.print_memory_status("åˆ†æå®Œæˆ")
    
    return results, fig_axes
